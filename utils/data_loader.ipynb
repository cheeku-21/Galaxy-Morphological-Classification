import tensorflow as tf

def create_datasets(batch_size=64):
    train_ds = tf.keras.utils.image_dataset_from_directory(
        "C:/Users/Sudha/OneDrive/Documents/GitHub/Projects/Galaxy Morphological Classification/data/processed/train",
        image_size=(128, 128),
        batch_size=batch_size,
        label_mode='categorical'
    )
    
    val_ds = tf.keras.utils.image_dataset_from_directory(
        "data/processed/val",
        image_size=(128, 128),
        batch_size=batch_size,
        label_mode='categorical'
    )
    
    test_ds = tf.keras.utils.image_dataset_from_directory(
        "data/processed/test",
        image_size=(128, 128),
        batch_size=batch_size,
        label_mode='categorical'
    )

    # Augmentation pipeline [9]
    augmentation = tf.keras.Sequential([
        tf.keras.layers.RandomFlip("horizontal_and_vertical"),
        tf.keras.layers.RandomRotation(0.3),
        tf.keras.layers.RandomContrast(0.2)
    ])

    # Configure datasets
    train_ds = train_ds.map(
        lambda x, y: (augmentation(x, training=True), y),
        num_parallel_calls=tf.data.AUTOTUNE
    ).prefetch(tf.data.AUTOTUNE)
    
    return train_ds, val_ds, test_ds
